{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.196152422706632\n"
     ]
    }
   ],
   "source": [
    "## Euclidean Distance\n",
    "#  - distance between two vectors\n",
    "import math\n",
    "def euclidean_distance(u, v):\n",
    "    summation = 0\n",
    "    for i in range(len(u)):\n",
    "        # (u1 - v1)^ 2\n",
    "        summation += ((u[i] - v[i]) * (u[i] - v[i]))\n",
    "    # squareroot result of summation\n",
    "    euclidean_distance = math.sqrt(summation)\n",
    "    return euclidean_distance\n",
    "# Example       \n",
    "print(euclidean_distance([1,2,3], [4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Done loading Google's Pre-Trained Word2Vec model\n"
     ]
    }
   ],
   "source": [
    "## Load Google's Pre-Trained Dataset\n",
    "##\n",
    "import gensim.models.word2vec as word2vec\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Google's Pre-Trained Data Set\n",
    "from gensim.models import KeyedVectors\n",
    "file_directory = '../Google_PreTrain_Dataset/slim/GoogleNews-vectors-negative300-SLIM.bin.gz'\n",
    "# Googles Pre-trained data set has 300 futures\n",
    "model = KeyedVectors.load_word2vec_format(file_directory, binary=True)\n",
    "\n",
    "print(\"## Done loading Google's Pre-Trained Word2Vec model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = 'President greets the press in Chicago'\n",
    "sentence_2 = 'Obama speaks to the media in Illinois'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1_normalized = 'president greets press chicago' \n",
    "sentence_2_normalized = 'leader speaks media illinois' \n",
    "sentence_3_normalized = 'leader speaks illinois' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.760273472991956\n",
      "3.376262869934666\n",
      "3.4098573759368542\n",
      "3.789893229351668\n"
     ]
    }
   ],
   "source": [
    "d1 = euclidean_distance(model[\"speaks\"], model[\"chicago\"])\n",
    "d2 = euclidean_distance(model[\"speaks\"], model[\"press\"])\n",
    "d3 = euclidean_distance(model[\"speaks\"], model[\"greets\"])\n",
    "d4 = euclidean_distance(model[\"speaks\"], model[\"president\"])\n",
    "\n",
    "print( d1 )\n",
    "print( d2 )\n",
    "print( d3 )\n",
    "print( d4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize both normalized sentences being compared\n",
    "##\n",
    "from nltk.tokenize import word_tokenize as tokenize_sentence\n",
    "\n",
    "# before tokenizing a sentence, the sentence has to be normalized\n",
    "sentence_1_tokenized = tokenize_sentence(sentence_1_normalized)\n",
    "sentence_2_tokenized = tokenize_sentence(sentence_2_normalized)\n",
    "\n",
    "print(sentence_2_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word embedding\n",
    "##\n",
    "\n",
    "# Using Google's pre-train dataset, convert every word in the tokenized sentence into its \n",
    "# position coordinates in a 300 dimension vector space\n",
    "def word_embedding(tokenized_sentence):\n",
    "    list_wordembedding = []\n",
    "    for token in tokenized_sentence:\n",
    "        list_wordembedding.append(model[token]) \n",
    "    return list_wordembedding\n",
    "        \n",
    "# word_embedding() takes in a tokenized sentence \n",
    "# this function will return a list of list containing\n",
    "# the vector position of every word in a sentece\n",
    "word_embedded_sentence_1 = word_embedding(sentence_1_tokenized)\n",
    "word_embedded_sentence_2 = word_embedding(sentence_2_tokenized)\n",
    "\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
