{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from gensim import corpora\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import TfidfModel\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from xlrd import open_workbook\n",
    "import operator\n",
    "import json\n",
    "import pandas as pd\n",
    "from gensim.models import LsiModel\n",
    "from gensim import similarities\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.corpora import MmCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[43, 40, 159]\n",
      "['C:\\\\Users\\\\z12dr\\\\AppData\\\\Local\\\\Temp\\\\dictionary1', 'C:\\\\Users\\\\z12dr\\\\AppData\\\\Local\\\\Temp\\\\dictionary2', 'C:\\\\Users\\\\z12dr\\\\AppData\\\\Local\\\\Temp\\\\dictionary3']\n",
      "['C:\\\\Users\\\\z12dr\\\\AppData\\\\Local\\\\Temp\\\\corpus1', 'C:\\\\Users\\\\z12dr\\\\AppData\\\\Local\\\\Temp\\\\corpus2', 'C:\\\\Users\\\\z12dr\\\\AppData\\\\Local\\\\Temp\\\\corpus3']\n"
     ]
    }
   ],
   "source": [
    "'''book = open_workbook('UNSPSC English v220601 project.xlsx')'''\n",
    "book = open_workbook('Unspec List2b.xlsx')\n",
    "'''To work on the UNSPSC sheet you need to change the values of 0 to 12 and 1 to\n",
    "16 in order to make the it work.'''\n",
    "dict_list = []\n",
    "sheet = book.sheet_by_index(0)\n",
    "# read header values into the list\n",
    "keys = [sheet.cell(0, col_index).value for col_index in range(sheet.ncols)]\n",
    "\n",
    "for row_index in range(1, sheet.nrows):\n",
    "    d = {keys[col_index]: sheet.cell(row_index, col_index).value\n",
    "         for col_index in range(sheet.ncols)}\n",
    "    dict_list.append(d)\n",
    "\n",
    "listOfEntry = []\n",
    "FamilyList = []\n",
    "sen = []\n",
    "counter = 1\n",
    "num=1\n",
    "\n",
    "path = \"UnSpec Family\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "\n",
    "for entry in dict_list:\n",
    "    if(entry.get(\"Family\")==\"\"):\n",
    "        continue\n",
    "    \n",
    "    if(entry.get(\"Family\")!=\"\" and entry.get(\"Class\")==\"\"):\n",
    "        #if(counter%2==0):\n",
    "        if(listOfEntry!=[]):\n",
    "            f = open(os.path.join(path,\"family{0}.txt\".format(num)),\"w\")\n",
    "            for line in listOfEntry:\n",
    "              hold = str(line,).strip('[]')\n",
    "              hold = hold.replace(\"'\",\"\")\n",
    "              #hold = str(str(line).strip('[]'),encoding='utf-8')\n",
    "              f.write(json.dumps(hold))\n",
    "              f.write(\"\\n\")\n",
    "            #f.writelines(json.dumps(listOfEntry))\n",
    "            counter += 1\n",
    "            f.close()\n",
    "            listOfEntry.clear()\n",
    "            continue\n",
    "\n",
    "        #else:\n",
    "         #   counter += 1\n",
    "        \n",
    "    else:\n",
    "        e = entry.get(\"Class Title\")\n",
    "        f = entry.get(\"Class Definition\")\n",
    "        g = entry.get(\"Commodity Title\")\n",
    "        h = entry.get(\"Commodity Definition\")\n",
    "        i = entry.get(\"Family\")\n",
    "        if(f != \"\" and h != \"\"):\n",
    "            result = e+\". \"+f+\". \"+g+\". \"+h+\". \"\n",
    "        elif(f != \"\" and h == \"\"):\n",
    "            result = e+\". \"+f+\". \"+g+\". \"+h\n",
    "        elif(f == \"\" and h != \"\"):\n",
    "            result = e+\". \"+f+\" \"+g+\". \"+h+\". \"\n",
    "        else:\n",
    "            result = e + \". \" + f + \"\" + g + \". \" + h\n",
    "        sen = sent_tokenize(result.lower())\n",
    "        listOfEntry.append(sen)\n",
    "        num=int(i)\n",
    "        e, f, g, h = \"\", \"\", \"\", \"\"\n",
    "\n",
    "        \n",
    "f = open(os.path.join(path,\"family{0}.txt\".format(num)),\"w+\")\n",
    "for line in listOfEntry:\n",
    "    hold = str(line,).strip('[]')\n",
    "    hold = hold.replace(\"'\",\"\")\n",
    "    f.write(json.dumps(hold))\n",
    "    f.write(\"\\n\")\n",
    "counter += 1\n",
    "f.close()\n",
    "        \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file_docs = []\n",
    "\n",
    "\n",
    "anotherCount = 1\n",
    "df1 = pd.DataFrame()\n",
    "df2 = []\n",
    "fam = []\n",
    "path1 = os.path.join(path)\n",
    "for file in os.listdir(path1):\n",
    "    with open(os.path.join(path,file),'r', encoding='utf-8') as infile:\n",
    "        txt = infile.readlines()\n",
    "        for i in txt:\n",
    "            fam.append(i) \n",
    "        df1 = df1.append(fam)\n",
    "        df2.append(df1)\n",
    "        df1 = pd.DataFrame()\n",
    "        fam.clear()\n",
    "        anotherCount += 1\n",
    "       \n",
    "alist =[]  \n",
    "\n",
    "for data in df2:\n",
    "    alist.append(data[0].tolist())\n",
    "    \n",
    "print(len(alist))\n",
    "\n",
    "token =[]\n",
    "\n",
    "\n",
    "Corlist=[]\n",
    "Dictlist=[]\n",
    "lengths=[]\n",
    "inc = 1\n",
    "ch = [ '``','``', \"''\",',','.','\\\\n',\"'\",\";\",\":\",\"(\",\")\",\"-\",\"--\"]\n",
    "for input in alist:\n",
    "    for j in input:\n",
    "        tokens = word_tokenize(j)\n",
    "        tokens = [ j for j in tokens if not j in ch ]\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        tokens = [p.replace('.',\"\") for p in tokens]\n",
    "        token.append(tokens)\n",
    "    out_fname = get_tmpfile(\"corpus{0}\".format(inc))\n",
    "    tmp_fname = get_tmpfile(\"dictionary{0}\".format(inc))\n",
    "    lengths.append(len(token))\n",
    "    inc += 1\n",
    "    dictionary = corpora.Dictionary(token)\n",
    "    corpus = [dictionary.doc2bow(entry) for entry in token]\n",
    "    token.clear()\n",
    "    tf_idf = TfidfModel(corpus)\n",
    "    cor_tf = tf_idf[corpus]\n",
    "    MmCorpus.serialize(out_fname,cor_tf)\n",
    "    Corlist.append(out_fname)\n",
    "    dictionary.save_as_text(tmp_fname)\n",
    "    Dictlist.append(tmp_fname)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(lengths)\n",
    "print(Dictlist)\n",
    "print(Corlist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(39, 0.3882386), (13, 0.33103013), (26, 0.13701919), (37, 0.091094926), (18, 0.07814352)]\n",
      "[(24, 0.99494433), (26, 0.5697041), (25, 0.5440636), (27, 0.5440636), (31, 0.20750782)]\n",
      "[(50, 0.5442184), (31, 0.48754525), (119, 0.43974388), (85, 0.40579844), (103, 0.37342858)]\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z12dr\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:430: RuntimeWarning: invalid value encountered in greater\n",
      "  nnz = np.nonzero(abs(vec) > eps)[0]\n"
     ]
    }
   ],
   "source": [
    "test = \"MASS TRANSPORTATION - RAIL VEHICLE PARTS AND ACCESSORIES\"\n",
    "test = test.lower()\n",
    "query_doc = word_tokenize(test)\n",
    "\n",
    "\n",
    "listNew = []\n",
    "\n",
    "\n",
    "def reduceList(lengths, dictionarys,corpuses):\n",
    "    for i in range(len(lengths)):\n",
    "        mm = MmCorpus(corpuses[i])\n",
    "        load_dic = corpora.Dictionary.load_from_text(dictionarys[i])\n",
    "        lsi = LsiModel(mm,num_topics=lengths[i],id2word = load_dic)\n",
    "        index = gensim.similarities.MatrixSimilarity(lsi[mm],num_features=lengths[i])\n",
    "        query_doc_bow = load_dic.doc2bow(query_doc, True)\n",
    "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        sim = index[lsi[query_doc_tf_idf]]\n",
    "        sim = sorted(enumerate(sim), key=lambda item: -item[1])\n",
    "        listNew.append(sim[0:5])\n",
    "        print(sim[0:5])\n",
    "\n",
    "\n",
    "reduceList(lengths,Dictlist,Corlist)\n",
    "    \n",
    "       \n",
    "\n",
    "print(len(listNew))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(39, 0.3882386), (13, 0.33103013), (26, 0.13701919), (37, 0.091094926), (18, 0.07814352)], [(24, 0.99494433), (26, 0.5697041), (25, 0.5440636), (27, 0.5440636), (31, 0.20750782)], [(50, 0.5442184), (31, 0.48754525), (119, 0.43974388), (85, 0.40579844), (103, 0.37342858)]]\n"
     ]
    }
   ],
   "source": [
    "print(listNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
