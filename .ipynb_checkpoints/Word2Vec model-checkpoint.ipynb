{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from xlrd import open_workbook\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from xlrd import open_workbook\n",
    "import json\n",
    "from gensim import similarities\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156478\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "book = open_workbook('UNSPSC English v220601 project.xlsx')\n",
    "'''book = open_workbook('Unspec List2b.xlsx')'''\n",
    "'''To work on the UNSPSC sheet you need to change the values of 0 to 12 and 1 to\n",
    "16 in order to make the it work.'''\n",
    "dict_list = []\n",
    "sheet = book.sheet_by_index(0)\n",
    "# read header values into the list\n",
    "keys = [sheet.cell(0, col_index).value for col_index in range(sheet.ncols)]\n",
    "\n",
    "for row_index in range(1, sheet.nrows):\n",
    "    d = {keys[col_index]: sheet.cell(row_index, col_index).value\n",
    "         for col_index in range(sheet.ncols)}\n",
    "    dict_list.append(d)\n",
    "\n",
    "print(len(dict_list))\n",
    "    \n",
    "doclist =[]\n",
    "listOfEntry =[]\n",
    "entrylist = []\n",
    "df7 = pd.DataFrame()\n",
    "\n",
    "\n",
    "for entry in dict_list:\n",
    "    if(entry.get(\"Family\")==\"\"):\n",
    "        continue\n",
    "    elif(entry.get(\"Family\")!=\"\" and entry.get(\"Class\")==\"\"):\n",
    "        if(listOfEntry!=[]):\n",
    "            #print(listOfEntry)\n",
    "            doclist += entrylist\n",
    "            continue\n",
    "    elif(entry.get(\"Commodity\")==\"\"):\n",
    "        continue\n",
    "    else:\n",
    "        if(entry.get(\"Class Definition\")!=\"\"):\n",
    "            f = str(sent_tokenize(entry.get(\"Class Definition\").lower())).strip('[]').strip('\\'')\n",
    "            #print(f)\n",
    "        else:\n",
    "            f=str([\" \"]).strip('[]').strip('\\'')\n",
    "        e = str(sent_tokenize(entry.get(\"Class Title\").lower())).strip('[]').strip('\\'')\n",
    "        #print(e)\n",
    "        g = str(sent_tokenize(entry.get(\"Commodity Title\").lower())).strip('[]').strip('\\'')\n",
    "        #print(g)\n",
    "        if(entry.get(\"Commodity Definition\")!=\"\"):\n",
    "            h = str(sent_tokenize(entry.get(\"Commodity Definition\").lower())).strip('[]').strip('\\'')\n",
    "            #print(h)\n",
    "        else:\n",
    "            h = str([\" \"]).strip('[]').strip('\\'')\n",
    "        i=int(entry.get(\"Commodity\"))\n",
    "        df7 = df7.append({'class':e,'CD':f,'commodity':g,'commod d':h,'level 1': i},ignore_index=True)\n",
    "        \n",
    "  \n",
    "        \n",
    "doclist.append(listOfEntry) \n",
    "\n",
    "print(\"done\")        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df8 = df7[['class','CD','commodity','commod d','level 1']]\n",
    "df9 = df8.apply(lambda x: ','.join(x.astype(str)),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean2 = pd.DataFrame({'clean':df9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent23 = [row.split(',') for row in df_clean2['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = gensim.models.Word2Vec(sent23,min_count=1,size=50, workers=4, window= 4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00182302,  0.00584302, -0.00556345,  0.0401251 ,  0.03000141,\n",
       "       -0.01957028,  0.0482937 , -0.03976969,  0.01302576, -0.06264427,\n",
       "       -0.05717637,  0.05124683,  0.02417065,  0.01963499,  0.02552753,\n",
       "        0.04932994,  0.02840692, -0.00596689,  0.00215287, -0.00162202,\n",
       "        0.05288649, -0.0044238 ,  0.0202107 , -0.02254551,  0.01705031,\n",
       "        0.03183617,  0.02602821, -0.04430541, -0.00710907, -0.05208826,\n",
       "       -0.0302466 ,  0.01645919,  0.08015165,  0.03563029, -0.0166442 ,\n",
       "       -0.01744538, -0.00323513,  0.01220475,  0.03031545,  0.09812513,\n",
       "       -0.0039107 ,  0.01887478,  0.04728105, -0.01779117,  0.01077457,\n",
       "       -0.02758724,  0.01677432,  0.00895405, -0.06474574, -0.02162056],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.__getitem__('39101601.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87909156"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.similarity('39101601.0','39101603.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.wv.vocab\n",
    "model2.save(\"W2V-Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mass', 'transportation', 'rail', 'vehicle', 'parts', 'accessories', '101']]\n"
     ]
    }
   ],
   "source": [
    "text = \"MASS TRANSPORTATION - RAIL VEHICLE PARTS AND ACCESSORIES 101\"\n",
    "text = text.lower()\n",
    "newEntry = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ch = [ '``','``', \"''\",',','.','\\\\n',\"'\",\";\",\":\",\"(\",\")\",\"-\",\"--\"]\n",
    "\n",
    "\n",
    "Wtest = word_tokenize(text)\n",
    "Wtest = [ j for j in Wtest if not j in ch ]\n",
    "Wtest = [w for w in Wtest if not w in stop_words]\n",
    "newEntry.append(Wtest)\n",
    "\n",
    "print(newEntry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0073818043"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mod = gensim.models.Word2Vec.load(\"W2V-Model\")\n",
    "new_mod.build_vocab(newEntry,update = True)\n",
    "new_mod.train(newEntry,total_examples=1,epochs = 1)\n",
    "\n",
    "new_mod.wv.similarity('39101601.0','101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('42181542.0', 0.30503520369529724),\n",
       " ('42143116.0', 0.2987354099750519),\n",
       " ('42144502.0', 0.2929975986480713),\n",
       " ('42142914.0', 0.2926304042339325),\n",
       " ('42142908.0', 0.2922489047050476),\n",
       " ('42203430.0', 0.2922215461730957),\n",
       " ('42131704.0', 0.28863805532455444),\n",
       " ('42131514.0', 0.2820744514465332),\n",
       " ('42121701.0', 0.28138676285743713),\n",
       " ('42172002.0', 0.2790583372116089)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mod.wv.most_similar('101')[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('42181542.0', 0.6525170207023621),\n",
       " ('42143116.0', 0.649367094039917),\n",
       " ('42144502.0', 0.6464982032775879),\n",
       " ('42142914.0', 0.6463146209716797),\n",
       " ('42142908.0', 0.6461238861083984),\n",
       " ('42203430.0', 0.6461101770401001),\n",
       " ('42131704.0', 0.6443184614181519),\n",
       " ('42131514.0', 0.6410366296768188),\n",
       " ('42121701.0', 0.6406927704811096),\n",
       " ('42172002.0', 0.6395285725593567)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mod.wv.most_similar_cosmul('101')[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
