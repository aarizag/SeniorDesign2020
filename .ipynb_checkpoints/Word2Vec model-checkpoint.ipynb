{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from xlrd import open_workbook\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import pprint\n",
    "import nltk\n",
    "import re\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from xlrd import open_workbook\n",
    "import json\n",
    "from gensim import similarities\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "       \n",
    "unspec_df = pd.read_excel('ignore/UNSPSC English v220601 project.xlsx','Sheet1', index_col=None, na_values=['NA'])\n",
    "unspec = unspec_df.drop(['Family Definition','Family','Family Title','Segment Title','Segment Definition','Segment','Version','Key','Synonym','Acronym'],axis = 1)\n",
    "cols = ['Class Title','Class Definition']\n",
    "cols2 =['Commodity Title','Commodity Definition']\n",
    "col3 = ['class','commodity']\n",
    "unspec[\"class\"] = unspec[cols].apply(lambda row: ','.join(row.values.astype(str)), axis=1)\n",
    "unspec[\"commodity\"] = unspec[cols2].apply(lambda row: ','.join(row.values.astype(str)), axis=1)\n",
    "unspec[\"combine\"]= unspec[col3].apply(lambda row: ', '.join(row.values.astype(str)),axis=1)\n",
    "unspec[\"combine\"]=[row.replace('nan','')for row in unspec[\"combine\"]]\n",
    "unspec2 = unspec.drop(['Class','Class Title','Class Definition','Commodity Title','Commodity Definition','class','commodity'],axis=1)\n",
    "unspec2=unspec2.rename(columns={\"Commodity\":\"level_1\"})\n",
    "nums = unspec2['level_1'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_df = pd.read_excel('ignore/eCAPS_COMM_11072019.xlsx','COMM_CLS', index_col=None, na_values=['NA'])\n",
    "county_df2 = pd.read_excel('ignore/eCAPS_COMM_11072019.xlsx','COMM_ITM', index_col=None, na_values=['NA'])\n",
    "county_df.drop(['COMM_GP','COMM_DET','ACT_INACT_FL','NAICS_CD'],axis=1)\n",
    "county_df2.drop(['COMM_GP','COMM_DET','ACT_INACT_FL','NAICS_CD'],axis=1)\n",
    "countyresult = pd.merge(county_df,county_df2,on='COMM_CLS')\n",
    "cr = countyresult.drop(['DSCR_EXT_y','DSCR_EXT_DV_x','DSCR_EXT_x','COMM_DSCR_UP_x','COMM_CD_x','COMM_ITM_x','COMM_GP_x','COMM_DET_x','ACT_INACT_FL_x','NAICS_CD_x','COMM_DET_y','COMM_GP_y','COMM_DET_y','ACT_INACT_FL_y','NAICS_CD_y','COMM_DSCR_UP_y','DSCR_EXT_DV_y'],axis=1)\n",
    "cr2 = cr.rename(columns={\"KEYWD_x\":\"class\",\"KEYWD_y\":\"commodity\",\"COMM_CD_y\":\"level_1\"})\n",
    "cr2[\"commodity\"]= [row.replace(' ',',')for row in cr2[\"commodity\"]]\n",
    "col4=[\"class\",\"commodity\"]\n",
    "cr2[\"combine\"]= cr2[col4].apply(lambda row: ', '.join(row.values.astype(str)),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1= cr2[['combine','level_1']]\n",
    "df2 = df1.append(unspec2,ignore_index=True,sort=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combine</th>\n",
       "      <th>level_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...</td>\n",
       "      <td>3510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...</td>\n",
       "      <td>3515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...</td>\n",
       "      <td>3520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...</td>\n",
       "      <td>3525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...</td>\n",
       "      <td>3526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             combine level_1\n",
       "0  AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...    3510\n",
       "1  AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...    3515\n",
       "2  AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...    3520\n",
       "3  AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...    3525\n",
       "4  AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...    3526"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df2.apply(lambda x: ','.join(x.astype(str)),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...\n",
       "1    AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...\n",
       "2    AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...\n",
       "3    AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...\n",
       "4    AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean2 = pd.DataFrame({'clean':df9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean\n",
       "0  AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...\n",
       "1  AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...\n",
       "2  AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...\n",
       "3  AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU...\n",
       "4  AIRCRAFT AND AIRPORT, EQUIPMENT, PARTS, AND SU..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent23 = [row.split(',') for row in df_clean2['clean']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"'Fresh'\", \" 'cut'\", \" 'amaranthuses'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'blooms'\", \" 'of'\", \" 'amaranthuses'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'upright'\", \" 'green'\", \" 'amaranthus'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'bloom'\", \" 'of'\", \" 'the'\", \" 'upright'\", \" 'green'\", \" 'variety'\", \" 'of'\", \" 'amaranthus'\", '10311804'], [\"'Fresh'\", \" 'cut'\", \" 'amaranthuses'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'blooms'\", \" 'of'\", \" 'amaranthuses'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'upright'\", \" 'red'\", \" 'amaranthus'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'bloom'\", \" 'of'\", \" 'the'\", \" 'upright'\", \" 'red'\", \" 'variety'\", \" 'of'\", \" 'amaranthus'\", '10311805'], [\"'Fresh'\", \" 'cut'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'blooms'\", \" 'of'\", \" 'amaryllises'\", \" '\", \"'\", \" 'nan'\", \" '\", \"'\", \" 'nan'\", 'nan'], [\"'Fresh'\", \" 'cut'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'blooms'\", \" 'of'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'naranja'\", \" 'amaryllis'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'bloom'\", \" 'of'\", \" 'the'\", \" 'naranja'\", \" 'variety'\", \" 'of'\", \" 'amaryllises'\", '10311901'], [\"'Fresh'\", \" 'cut'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'blooms'\", \" 'of'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'orange'\", \" 'nagano'\", \" 'amaryllis'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'bloom'\", \" 'of'\", \" 'the'\", \" 'orange'\", \" 'nagano'\", \" 'variety'\", \" 'of'\", \" 'amaryllises'\", '10311902'], [\"'Fresh'\", \" 'cut'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'blooms'\", \" 'of'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'pygmee'\", \" 'mini'\", \" 'amaryllis'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'bloom'\", \" 'of'\", \" 'the'\", \" 'pygmee'\", \" 'mini'\", \" 'variety'\", \" 'of'\", \" 'amaryllises'\", '10311903'], [\"'Fresh'\", \" 'cut'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'blooms'\", \" 'of'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'red'\", \" 'lion'\", \" 'amaryllis'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'bloom'\", \" 'of'\", \" 'the'\", \" 'red'\", \" 'lion'\", \" 'variety'\", \" 'of'\", \" 'amaryllises'\", '10311904'], [\"'Fresh'\", \" 'cut'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'blooms'\", \" 'of'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'rilona'\", \" 'amaryllis'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'bloom'\", \" 'of'\", \" 'the'\", \" 'rilona'\", \" 'variety'\", \" 'of'\", \" 'amaryllises'\", '10311905'], [\"'Fresh'\", \" 'cut'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'blooms'\", \" 'of'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'royal'\", \" 'velvet'\", \" 'amaryllis'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'bloom'\", \" 'of'\", \" 'the'\", \" 'royal'\", \" 'velvet'\", \" 'variety'\", \" 'of'\", \" 'amaryllises'\", '10311906'], [\"'Fresh'\", \" 'cut'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'blooms'\", \" 'of'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'sonatini'\", \" 'orange'\", \" 'amaryllis'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'bloom'\", \" 'of'\", \" 'the'\", \" 'sonatini'\", \" 'orange'\", \" 'variety'\", \" 'of'\", \" 'amaryllises'\", '10311907'], [\"'Fresh'\", \" 'cut'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'blooms'\", \" 'of'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'sonatini'\", \" 'red'\", \" 'amaryllis'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'bloom'\", \" 'of'\", \" 'the'\", \" 'sonatini'\", \" 'red'\", \" 'variety'\", \" 'of'\", \" 'amaryllises'\", '10311908'], [\"'Fresh'\", \" 'cut'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'blooms'\", \" 'of'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'tango'\", \" 'amaryllis'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'bloom'\", \" 'of'\", \" 'the'\", \" 'tango'\", \" 'variety'\", \" 'of'\", \" 'amaryllises'\", '10311909'], [\"'Fresh'\", \" 'cut'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'blooms'\", \" 'of'\", \" 'amaryllises'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'tinto'\", \" 'night'\", \" 'amaryllis'\", \" '\", \"'\", \" 'Fresh'\", \" 'cut'\", \" 'bloom'\", \" 'of'\", \" 'the'\", \" 'tinto'\", \" 'night'\", \" 'variety'\", \" 'of'\", \" 'amaryllises'\", '10311910']]\n"
     ]
    }
   ],
   "source": [
    "print(sent23[10542:10555])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163115\n"
     ]
    }
   ],
   "source": [
    "print(len(sent23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = gensim.models.word2vec.Word2Vec(sent23,min_count=1,size=300, workers=6, window= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51529\n",
      "                                                  combine   level_1\n",
      "6640     'Livestock', ',', 'nan', ',', 'Cats', ',', 'nan'  10101501\n",
      "6641     'Livestock', ',', 'nan', ',', 'Dogs', ',', 'nan'  10101502\n",
      "6642     'Livestock', ',', 'nan', ',', 'Mink', ',', 'nan'  10101504\n",
      "6643     'Livestock', ',', 'nan', ',', 'Rats', ',', 'nan'  10101505\n",
      "6644    'Livestock', ',', 'nan', ',', 'Horses', ',', '...  10101506\n",
      "6645    'Livestock', ',', 'nan', ',', 'Sheep', ',', 'nan'  10101507\n",
      "6646    'Livestock', ',', 'nan', ',', 'Goats', ',', 'nan'  10101508\n",
      "6647    'Livestock', ',', 'nan', ',', 'Asses', ',', 'nan'  10101509\n",
      "6648     'Livestock', ',', 'nan', ',', 'Mice', ',', 'nan'  10101510\n",
      "6649    'Livestock', ',', 'nan', ',', 'Swine', ',', 'nan'  10101511\n",
      "6650    'Livestock', ',', 'nan', ',', 'Rabbits', ',', ...  10101512\n",
      "6651    'Livestock', ',', 'nan', ',', 'Guinea', 'pigs'...  10101513\n",
      "6652    'Livestock', ',', 'nan', ',', 'Primates', ',',...  10101514\n",
      "6653    'Livestock', ',', 'nan', ',', 'Armadillos', ',...  10101515\n",
      "6654    'Livestock', ',', 'nan', ',', 'Cattle', ',', '...  10101516\n",
      "6655    'Livestock', ',', 'nan', ',', 'Camels', ',', '...  10101517\n",
      "6656    'Livestock', ',', 'nan', ',', 'Alpaca', ',', '...  10101518\n",
      "6657    'Livestock', ',', 'nan', ',', 'Buffalo', 'or',...  10101519\n",
      "6658    'Livestock', ',', 'nan', ',', 'Llama', ',', 'T...  10101520\n",
      "6660    'Birds', 'and', 'fowl', ',', 'nan', ',', 'Live...  10101601\n",
      "6662    'Birds', 'and', 'fowl', ',', 'nan', ',', 'Live...  10101603\n",
      "6664    'Birds', 'and', 'fowl', ',', 'nan', ',', 'Live...  10101605\n",
      "6665    'Birds', 'and', 'fowl', ',', 'nan', ',', 'Live...  10101606\n",
      "6668    'Live', 'fish', ',', 'nan', ',', 'Live', 'salm...  10101701\n",
      "6669    'Live', 'fish', ',', 'nan', ',', 'Live', 'trou...  10101702\n",
      "6670    'Live', 'fish', ',', 'nan', ',', 'Live', 'tila...  10101703\n",
      "6671    'Live', 'fish', ',', 'nan', ',', 'Live', 'carp...  10101704\n",
      "6672    'Live', 'fish', ',', 'nan', ',', 'Live', 'eels...  10101705\n",
      "6675    'Live', 'fish', ',', 'nan', ',', 'Live', 'sard...  10101708\n",
      "6676    'Live', 'fish', ',', 'nan', ',', 'Live', 'red'...  10101709\n",
      "...                                                   ...       ...\n",
      "163055  'Agricultural', 'and', 'farming', 'and', 'fish...  95122504\n",
      "163060  'Religious', 'buildings', 'and', 'structures',...  95122603\n",
      "163061  'Religious', 'buildings', 'and', 'structures',...  95122604\n",
      "163064  'Defense', 'buildings', 'and', 'structures', '...  95122701\n",
      "163065  'Defense', 'buildings', 'and', 'structures', '...  95122702\n",
      "163066  'Defense', 'buildings', 'and', 'structures', '...  95122703\n",
      "163073  'Portable', 'commercial', 'and', 'industrial',...  95131601\n",
      "163075  'Portable', 'commercial', 'and', 'industrial',...  95131603\n",
      "163076  'Portable', 'commercial', 'and', 'industrial',...  95131604\n",
      "163078  'Portable', 'commercial', 'and', 'industrial',...  95131606\n",
      "163085  'Prefabricated', 'farm', 'buildings', 'and', '...  95141502\n",
      "163088  'Prefabricated', 'residential', 'buildings', '...  95141602\n",
      "163090  'Prefabricated', 'residential', 'buildings', '...  95141604\n",
      "163091  'Prefabricated', 'residential', 'buildings', '...  95141605\n",
      "163092  'Prefabricated', 'residential', 'buildings', '...  95141606\n",
      "163094  'Prefabricated', 'commercial', 'and', 'industr...  95141701\n",
      "163095  'Prefabricated', 'commercial', 'and', 'industr...  95141702\n",
      "163096  'Prefabricated', 'commercial', 'and', 'industr...  95141703\n",
      "163097  'Prefabricated', 'commercial', 'and', 'industr...  95141704\n",
      "163098  'Prefabricated', 'commercial', 'and', 'industr...  95141705\n",
      "163099  'Prefabricated', 'commercial', 'and', 'industr...  95141706\n",
      "163101  'Prefabricated', 'commercial', 'and', 'industr...  95141708\n",
      "163103  'Prefabricated', 'commercial', 'and', 'industr...  95141710\n",
      "163104  'Prefabricated', 'commercial', 'and', 'industr...  95141711\n",
      "163105  'Prefabricated', 'commercial', 'and', 'industr...  95141712\n",
      "163107  'Prefabricated', 'emergency', 'relief', 'build...  95141801\n",
      "163108  'Prefabricated', 'emergency', 'relief', 'build...  95141802\n",
      "163111  'Prefabricated', 'medical', 'buildings', 'and'...  95141901\n",
      "163112  'Prefabricated', 'medical', 'buildings', 'and'...  95141902\n",
      "163113  'Prefabricated', 'medical', 'buildings', 'and'...  95141903\n",
      "\n",
      "[51529 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def NumberofDocs(li,nums):\n",
    "    shortlist=[]\n",
    "    number=0.0\n",
    "    for i in li:\n",
    "        if(i!='nan'):\n",
    "            point = model2.wv.similarity(str(i),nums)\n",
    "            if(point>0.5):\n",
    "                shortlist.append(i) \n",
    "    \n",
    "    return shortlist\n",
    "\n",
    "result =NumberofDocs(nums,'4045') \n",
    "print(len(result))\n",
    "print(df2.loc[df2['level_1'].isin(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Empty DataFrame\n",
      "Columns: [combine, level_1]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "result2 =NumberofDocs(nums,'3526') \n",
    "print(len(result2))\n",
    "print(df2.loc[df2['level_1'].isin(result2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03278522"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.similarity('10101707','4045')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.38505948"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.similarity('85903723','4045')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\" '0RPS47Z'\", 0.9644993543624878),\n",
       " (\" '0RQ03ZZ'\", 0.9606325626373291),\n",
       " (\" '08W083Z'\", 0.959457278251648),\n",
       " ('RIGS', 0.9566643238067627),\n",
       " (\" '0C9WXZ0'\", 0.9555591344833374),\n",
       " (\" 'vinci'\", 0.9531040191650391),\n",
       " (\" 'DW11BBZ'\", 0.9524223804473877),\n",
       " (\" '0TNC7ZZ'\", 0.9520471096038818),\n",
       " (\" '0X020KZ'\", 0.951983630657196),\n",
       " ('17R', 0.9509916305541992)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv.most_similar('3105')[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbook = open_workbook(\\'UNSPSC English v220601 project.xlsx\\')\\n\\'\\'\\'book = open_workbook(\\'Unspec List2b.xlsx\\')\\'\\'\\'\\n\\'\\'\\'To work on the UNSPSC sheet you need to change the values of 0 to 12 and 1 to\\n16 in order to make the it work.\\'\\'\\'\\ndict_list = []\\nsheet = book.sheet_by_index(0)\\n# read header values into the list\\nkeys = [sheet.cell(0, col_index).value for col_index in range(sheet.ncols)]\\n\\nfor row_index in range(1, sheet.nrows):\\n    d = {keys[col_index]: sheet.cell(row_index, col_index).value\\n         for col_index in range(sheet.ncols)}\\n    dict_list.append(d)\\n\\nprint(len(dict_list))\\n    \\ndoclist =[]\\nlistOfEntry =[]\\nentrylist = []\\ndf7 = pd.DataFrame()\\n\\n\\nfor entry in dict_list:\\n    if(entry.get(\"Family\")==\"\"):\\n        continue\\n    elif(entry.get(\"Family\")!=\"\" and entry.get(\"Class\")==\"\"):\\n        if(listOfEntry!=[]):\\n            #print(listOfEntry)\\n            doclist += entrylist\\n            continue\\n    elif(entry.get(\"Commodity\")==\"\"):\\n        continue\\n    else:\\n        if(entry.get(\"Class Definition\")!=\"\"):\\n            f = str(sent_tokenize(entry.get(\"Class Definition\").lower())).strip(\\'[]\\').strip(\\'\\'\\')\\n            #print(f)\\n        else:\\n            f=str([\" \"]).strip(\\'[]\\').strip(\\'\\'\\')\\n        e = str(sent_tokenize(entry.get(\"Class Title\").lower())).strip(\\'[]\\').strip(\\'\\'\\')\\n        #print(e)\\n        g = str(sent_tokenize(entry.get(\"Commodity Title\").lower())).strip(\\'[]\\').strip(\\'\\'\\')\\n        #print(g)\\n        if(entry.get(\"Commodity Definition\")!=\"\"):\\n            h = str(sent_tokenize(entry.get(\"Commodity Definition\").lower())).strip(\\'[]\\').strip(\\'\\'\\')\\n            #print(h)\\n        else:\\n            h = str([\" \"]).strip(\\'[]\\').strip(\\'\\'\\')\\n        i=int(entry.get(\"Commodity\"))\\n        df7 = df7.append({\\'class\\':e+f,\\'commodity\\':g+h,\\'level_1\\': i},ignore_index=True)\\n        \\n  \\n        \\ndoclist.append(listOfEntry) \\n\\nprint(\"done\")        \\n\\n\\n\\n\\nmodel2.wv.vocab\\nmodel2.save(\"W2V-Model\")\\n\\ntext = \"AIR CONDITIONING, HEATING, AND VENTILATING: EQUIPMENT, PARTS  HYDRONIC SPECIALTIES\"\\nidt =[\\'101\\'] \\ntext = text.lower()\\nnewEntry = []\\nnewEn =[]\\nstop_words = set(stopwords.words(\\'english\\'))\\nch = [ \\'``\\',\\'``\\', \"\\'\\'\",\\',\\',\\'.\\',\\'\\\\n\\',\"\\'\",\";\",\":\",\"(\",\")\",\"-\",\"--\"]\\n\\n\\nWtest = word_tokenize(text)\\nWtest = [ j for j in Wtest if not j in ch ]\\nWtest = [w for w in Wtest if not w in stop_words]\\nnewEntry.append(Wtest)\\n\\ndef concatenate_list_data(list):\\n    result= \\'\\'\\n    for element in list:\\n        result += str(element+\" \")\\n    return result\\n\\nnewEntry[0] = concatenate_list_data(newEntry[0])\\nnewEntry += idt\\nnewEn.append(newEntry)\\nprint(newEn)\\n\\nnew_mod = gensim.models.Word2Vec.load(\"W2V-Model\")\\nnew_mod.build_vocab(newEn,update = True)\\nnew_mod.train(newEn,total_examples=1,epochs = 1)\\n\\nnew_mod.wv.similarity(\\'30121703.0\\',\\'101\\')\\nnew_mod.wv.most_similar_cosmul(\\'101\\')\\n\\nnew_mod.wv.similar_by_vector(\\'101\\')\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "book = open_workbook('UNSPSC English v220601 project.xlsx')\n",
    "'''book = open_workbook('Unspec List2b.xlsx')'''\n",
    "'''To work on the UNSPSC sheet you need to change the values of 0 to 12 and 1 to\n",
    "16 in order to make the it work.'''\n",
    "dict_list = []\n",
    "sheet = book.sheet_by_index(0)\n",
    "# read header values into the list\n",
    "keys = [sheet.cell(0, col_index).value for col_index in range(sheet.ncols)]\n",
    "\n",
    "for row_index in range(1, sheet.nrows):\n",
    "    d = {keys[col_index]: sheet.cell(row_index, col_index).value\n",
    "         for col_index in range(sheet.ncols)}\n",
    "    dict_list.append(d)\n",
    "\n",
    "print(len(dict_list))\n",
    "    \n",
    "doclist =[]\n",
    "listOfEntry =[]\n",
    "entrylist = []\n",
    "df7 = pd.DataFrame()\n",
    "\n",
    "\n",
    "for entry in dict_list:\n",
    "    if(entry.get(\"Family\")==\"\"):\n",
    "        continue\n",
    "    elif(entry.get(\"Family\")!=\"\" and entry.get(\"Class\")==\"\"):\n",
    "        if(listOfEntry!=[]):\n",
    "            #print(listOfEntry)\n",
    "            doclist += entrylist\n",
    "            continue\n",
    "    elif(entry.get(\"Commodity\")==\"\"):\n",
    "        continue\n",
    "    else:\n",
    "        if(entry.get(\"Class Definition\")!=\"\"):\n",
    "            f = str(sent_tokenize(entry.get(\"Class Definition\").lower())).strip('[]').strip('\\'')\n",
    "            #print(f)\n",
    "        else:\n",
    "            f=str([\" \"]).strip('[]').strip('\\'')\n",
    "        e = str(sent_tokenize(entry.get(\"Class Title\").lower())).strip('[]').strip('\\'')\n",
    "        #print(e)\n",
    "        g = str(sent_tokenize(entry.get(\"Commodity Title\").lower())).strip('[]').strip('\\'')\n",
    "        #print(g)\n",
    "        if(entry.get(\"Commodity Definition\")!=\"\"):\n",
    "            h = str(sent_tokenize(entry.get(\"Commodity Definition\").lower())).strip('[]').strip('\\'')\n",
    "            #print(h)\n",
    "        else:\n",
    "            h = str([\" \"]).strip('[]').strip('\\'')\n",
    "        i=int(entry.get(\"Commodity\"))\n",
    "        df7 = df7.append({'class':e+f,'commodity':g+h,'level_1': i},ignore_index=True)\n",
    "        \n",
    "  \n",
    "        \n",
    "doclist.append(listOfEntry) \n",
    "\n",
    "print(\"done\")        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model2.wv.vocab\n",
    "model2.save(\"W2V-Model\")\n",
    "\n",
    "text = \"AIR CONDITIONING, HEATING, AND VENTILATING: EQUIPMENT, PARTS  HYDRONIC SPECIALTIES\"\n",
    "idt =['101'] \n",
    "text = text.lower()\n",
    "newEntry = []\n",
    "newEn =[]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ch = [ '``','``', \"''\",',','.','\\\\n',\"'\",\";\",\":\",\"(\",\")\",\"-\",\"--\"]\n",
    "\n",
    "\n",
    "Wtest = word_tokenize(text)\n",
    "Wtest = [ j for j in Wtest if not j in ch ]\n",
    "Wtest = [w for w in Wtest if not w in stop_words]\n",
    "newEntry.append(Wtest)\n",
    "\n",
    "def concatenate_list_data(list):\n",
    "    result= ''\n",
    "    for element in list:\n",
    "        result += str(element+\" \")\n",
    "    return result\n",
    "\n",
    "newEntry[0] = concatenate_list_data(newEntry[0])\n",
    "newEntry += idt\n",
    "newEn.append(newEntry)\n",
    "print(newEn)\n",
    "\n",
    "new_mod = gensim.models.Word2Vec.load(\"W2V-Model\")\n",
    "new_mod.build_vocab(newEn,update = True)\n",
    "new_mod.train(newEn,total_examples=1,epochs = 1)\n",
    "\n",
    "new_mod.wv.similarity('30121703.0','101')\n",
    "new_mod.wv.most_similar_cosmul('101')\n",
    "\n",
    "new_mod.wv.similar_by_vector('101')\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
