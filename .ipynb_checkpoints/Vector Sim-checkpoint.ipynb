{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from gensim import corpora\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import TfidfModel\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from xlrd import open_workbook\n",
    "import operator\n",
    "import json\n",
    "import pandas as pd\n",
    "from gensim.models import LsiModel\n",
    "from gensim import similarities\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 28\n",
      "LsiModel(num_terms=165, num_topics=28, decay=1.0, chunksize=20000)\n",
      "MatrixSimilarity<28 docs, 28 features>\n",
      "Dictionary(165 unique tokens: ['.', 'fireworks', 'computer', 'paper', 'printout']...)\n",
      "[(6, 0.9998356), (9, 2.9802322e-08), (7, 5.5879354e-09), (25, 5.5879354e-09), (0, 3.7252903e-09)]\n"
     ]
    }
   ],
   "source": [
    "book = open_workbook('Unspec List.xlsx')\n",
    "'''To work on the UNSPSC sheet you need to change the values of 0 to 12 and 1 to\n",
    "16 in order to make the it work.'''\n",
    "dict_list = []\n",
    "sheet = book.sheet_by_index(0)\n",
    "# read header values into the list\n",
    "keys = [sheet.cell(0, col_index).value for col_index in range(sheet.ncols)]\n",
    "\n",
    "for row_index in range(1, sheet.nrows):\n",
    "    d = {keys[col_index]: sheet.cell(row_index, col_index).value\n",
    "         for col_index in range(sheet.ncols)}\n",
    "    dict_list.append(d)\n",
    "\n",
    "listOfEntry = []\n",
    "segments = []\n",
    "FamilyList = []\n",
    "sen = []\n",
    "keyvaluePair = []\n",
    "counter = 1\n",
    "\n",
    "for entry in dict_list:\n",
    "    e = entry.get(\"Class Title\")\n",
    "    f = entry.get(\"Class Definition\")\n",
    "    g = entry.get(\"Commodity Title\")\n",
    "    h = entry.get(\"Commodity Definition\")\n",
    "    i = entry.get(\"Commodity\")\n",
    "    if(f != \"\" and h != \"\"):\n",
    "        result = e+\". \"+f+\". \"+g+\". \"+h+\". \"\n",
    "    elif(f != \"\" and h == \"\"):\n",
    "        result = e+\". \"+f+\". \"+g+\". \"+h\n",
    "    elif(f == \"\" and h != \"\"):\n",
    "        result = e+\". \"+f+\" \"+g+\". \"+h+\". \"\n",
    "    else:\n",
    "        result = e + \". \" + f + \"\" + g + \". \" + h\n",
    "    sen = sent_tokenize(result.lower())\n",
    "    listOfEntry.append(sen)\n",
    "    keyvaluePair.append(i)\n",
    "    e, f, g, h = \"\", \"\", \"\", \"\"\n",
    "    \n",
    "stop_words = set(stopwords.words('english'))\n",
    "file_docs = []\n",
    "\n",
    "for input in listOfEntry:\n",
    "    for j in input:\n",
    "        tokens = word_tokenize(j)\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "    file_docs.append(tokens)\n",
    "\n",
    "print(\"Number of documents:\", len(file_docs))  \n",
    "\n",
    "\"\"\"\n",
    "Dictionary is a mapping between normalized words and their integer IDs\n",
    "Token2id is a type of dict of(str, int).\n",
    "id2token is the same but is a lazy manner to save on memory.\n",
    "To get the total amount of corpus positions which is the number\n",
    "of processed words.\n",
    "\n",
    "\"\"\"\n",
    "dictionary = corpora.Dictionary(file_docs)\n",
    "\n",
    "\"\"\"\n",
    "In the coming function call we will be converting a document to a beg of words\n",
    "the tuple created is the token Id of the word and the token count for that word\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "corpus = [dictionary.doc2bow(entry) for entry in file_docs]\n",
    "\n",
    "tf_idf = TfidfModel(corpus)\n",
    "cor_tf = tf_idf[corpus]\n",
    "\n",
    "\n",
    "\n",
    "test = \"MASS TRANSPORTATION - RAIL VEHICLE PARTS AND ACCESSORIES\"\n",
    "test = test.lower()\n",
    "\n",
    "query_doc = word_tokenize(test)\n",
    "\n",
    "query_doc_bow = dictionary.doc2bow(query_doc, True)\n",
    "\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "\n",
    "numtopic = len(file_docs)\n",
    "\n",
    "lsi = LsiModel(cor_tf, num_topics=numtopic,id2word = dictionary)\n",
    "print(lsi)\n",
    "\n",
    "index = gensim.similarities.MatrixSimilarity(lsi[cor_tf], num_features=numtopic)\n",
    "print(index)\n",
    "sims = index[lsi[query_doc_tf_idf]]\n",
    "\n",
    "print(dictionary)\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])   \n",
    "\n",
    "\n",
    "print(sims[0:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
