{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 156478\n",
      "['mass', 'transportation', '-', 'rail', 'vehicle', 'parts', 'and', 'accessories']\n",
      "[(542, 1), (810, 1), (4095, 1), (5902, 1), (8236, 1), (8298, 1), (9780, 1), (108543, 1)]\n",
      "(78101905.0, 0.49936673)\n",
      "(78101902.0, 0.47229958)\n",
      "(86101715.0, 0.3773628)\n",
      "(30103101.0, 0.35129178)\n",
      "(40141616.0, 0.34450802)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' a = entry.get(\"Segment Title\")\\na.replace(\\'.\\',\" \")\\nb = entry.get(\"Segment Definition\")\\nb.replace(\\'.\\',\" \")\\nc = entry.get(\"Family Title\")\\nc.replace(\\'.\\',\" \")\\nd =entry.get(\"Family Definition\")\\nd.replace(\\'.\\',\" \")  '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk,re,pprint\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from xlrd import open_workbook\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "\n",
    "'''book = open_workbook('UNSPSC English v220601 project.xlsx')'''\n",
    "book = open_workbook('Unspec List.xlsx')\n",
    "'''To work on the UNSPSC sheet you need to change the values of 0 to 12 and 1 to\n",
    "16 in order to make the it work.'''\n",
    "dict_list = []\n",
    "sheet = book.sheet_by_index(0)\n",
    "# read header values into the list\n",
    "keys = [sheet.cell(0, col_index).value for col_index in range(sheet.ncols)]\n",
    "\n",
    "for row_index in range(1, sheet.nrows):\n",
    "    d = {keys[col_index]: sheet.cell(row_index, col_index).value\n",
    "         for col_index in range(sheet.ncols)}\n",
    "    dict_list.append(d)\n",
    "\n",
    "listOfEntry = []\n",
    "tokenSen = []\n",
    "sen = []\n",
    "keyvaluePair = []\n",
    "counter = 0\n",
    "\n",
    "for entry in dict_list:\n",
    "    e = entry.get(\"Class Title\")\n",
    "    f = entry.get(\"Class Definition\")\n",
    "    g = entry.get(\"Commodity Title\")\n",
    "    h = entry.get(\"Commodity Definition\")\n",
    "    i = entry.get(\"Commodity\")\n",
    "    if(f != \"\" and h != \"\"):\n",
    "        result = e+\". \"+f+\". \"+g+\". \"+h+\". \"\n",
    "    elif(f != \"\" and h == \"\"):\n",
    "        result = e+\". \"+f+\". \"+g+\". \"+h\n",
    "    elif(f == \"\" and h != \"\"):\n",
    "        result = e+\". \"+f+\" \"+g+\". \"+h+\". \"\n",
    "    else:\n",
    "        result = e + \". \" + f + \"\" + g + \". \" + h\n",
    "    sen = sent_tokenize(result.lower())\n",
    "    listOfEntry.append(sen)\n",
    "    keyvaluePair.append(i)\n",
    "    e, f, g, h = \"\", \"\", \"\", \"\"\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file_docs = []\n",
    "\n",
    "\n",
    "for input in listOfEntry:\n",
    "    for j in input:\n",
    "        tokens = word_tokenize(j)\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "    file_docs.append(tokens)\n",
    "\n",
    "print(\"Number of documents:\", len(file_docs))\n",
    "\n",
    "\"\"\"\n",
    "Dictionary is a mapping between normalized words and their integer IDs\n",
    "Token2id is a type of dict of(str, int).\n",
    "id2token is the same but is a lazy manner to save on memory.\n",
    "To get the total amount of corpus positions which is the number\n",
    "of processed words.\n",
    "\"\"\"\n",
    "dictionary = gensim.corpora.Dictionary(file_docs)\n",
    "\"\"\"\n",
    "In the coming function call we will be converting a document to a beg of words\n",
    "the tuple created is the token Id of the word and the token count for that word\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "corpus = [dictionary.doc2bow(entry) for entry in file_docs]\n",
    "\n",
    "\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "\n",
    "\n",
    "sims = gensim.similarities.Similarity('dir', tf_idf[corpus], num_features=len(dictionary))\n",
    "\n",
    "\n",
    "test = \"MASS TRANSPORTATION - RAIL VEHICLE PARTS AND ACCESSORIES\"\n",
    "test = test.lower()\n",
    "\n",
    "query_doc = word_tokenize(test)\n",
    "print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc, True)\n",
    "print(query_doc_bow)\n",
    "\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "\n",
    "\n",
    "\n",
    "listnew =[]\n",
    "listnew = list(zip(keyvaluePair,sims[query_doc_tf_idf]))\n",
    "\n",
    "listnew.sort(key = operator.itemgetter(1),reverse = True)\n",
    "\n",
    "for i in range(5):\n",
    "    print(listnew[i])\n",
    "\n",
    "\n",
    "\"\"\" a = entry.get(\"Segment Title\")\n",
    "a.replace('.',\" \")\n",
    "b = entry.get(\"Segment Definition\")\n",
    "b.replace('.',\" \")\n",
    "c = entry.get(\"Family Title\")\n",
    "c.replace('.',\" \")\n",
    "d =entry.get(\"Family Definition\")\n",
    "d.replace('.',\" \")  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 28\n",
      "['mass', 'transportation', '-', 'rail', 'vehicle', 'parts', 'and', 'accessories']\n",
      "[(25, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1)]\n",
      "(30121703.0, 0.7053112)\n",
      "(12131601.0, 0.0)\n",
      "(14111506.0, 0.0)\n",
      "(24101601.0, 0.0)\n",
      "(25111937.0, 0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' a = entry.get(\"Segment Title\")\\na.replace(\\'.\\',\" \")\\nb = entry.get(\"Segment Definition\")\\nb.replace(\\'.\\',\" \")\\nc = entry.get(\"Family Title\")\\nc.replace(\\'.\\',\" \")\\nd =entry.get(\"Family Definition\")\\nd.replace(\\'.\\',\" \")  '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk,re,pprint\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from xlrd import open_workbook\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "\n",
    "'''book = open_workbook('UNSPSC English v220601 project.xlsx')'''\n",
    "book = open_workbook('Unspec List.xlsx')\n",
    "'''To work on the UNSPSC sheet you need to change the values of 0 to 12 and 1 to\n",
    "16 in order to make the it work.'''\n",
    "dict_list = []\n",
    "sheet = book.sheet_by_index(0)\n",
    "# read header values into the list\n",
    "keys = [sheet.cell(0, col_index).value for col_index in range(sheet.ncols)]\n",
    "\n",
    "for row_index in range(1, sheet.nrows):\n",
    "    d = {keys[col_index]: sheet.cell(row_index, col_index).value\n",
    "         for col_index in range(sheet.ncols)}\n",
    "    dict_list.append(d)\n",
    "\n",
    "listOfEntry = []\n",
    "tokenSen = []\n",
    "sen = []\n",
    "keyvaluePair = []\n",
    "counter = 0\n",
    "\n",
    "for entry in dict_list:\n",
    "    e = entry.get(\"Class Title\")\n",
    "    f = entry.get(\"Class Definition\")\n",
    "    g = entry.get(\"Commodity Title\")\n",
    "    h = entry.get(\"Commodity Definition\")\n",
    "    i = entry.get(\"Commodity\")\n",
    "    if(f != \"\" and h != \"\"):\n",
    "        result = e+\". \"+f+\". \"+g+\". \"+h+\". \"\n",
    "    elif(f != \"\" and h == \"\"):\n",
    "        result = e+\". \"+f+\". \"+g+\". \"+h\n",
    "    elif(f == \"\" and h != \"\"):\n",
    "        result = e+\". \"+f+\" \"+g+\". \"+h+\". \"\n",
    "    else:\n",
    "        result = e + \". \" + f + \"\" + g + \". \" + h\n",
    "    sen = sent_tokenize(result.lower())\n",
    "    listOfEntry.append(sen)\n",
    "    keyvaluePair.append(i)\n",
    "    e, f, g, h = \"\", \"\", \"\", \"\"\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file_docs = []\n",
    "\n",
    "\n",
    "for input in listOfEntry:\n",
    "    for j in input:\n",
    "        tokens = word_tokenize(j)\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "    file_docs.append(tokens)\n",
    "\n",
    "print(\"Number of documents:\", len(file_docs))\n",
    "\n",
    "\"\"\"\n",
    "Dictionary is a mapping between normalized words and their integer IDs\n",
    "Token2id is a type of dict of(str, int).\n",
    "id2token is the same but is a lazy manner to save on memory.\n",
    "To get the total amount of corpus positions which is the number\n",
    "of processed words.\n",
    "\"\"\"\n",
    "dictionary = gensim.corpora.Dictionary(file_docs)\n",
    "\"\"\"\n",
    "In the coming function call we will be converting a document to a beg of words\n",
    "the tuple created is the token Id of the word and the token count for that word\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "corpus = [dictionary.doc2bow(entry) for entry in file_docs]\n",
    "\n",
    "\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "\n",
    "\n",
    "sims = gensim.similarities.Similarity('dir', tf_idf[corpus], num_features=len(dictionary))\n",
    "\n",
    "\n",
    "test = \"MASS TRANSPORTATION - RAIL VEHICLE PARTS AND ACCESSORIES\"\n",
    "test = test.lower()\n",
    "\n",
    "query_doc = word_tokenize(test)\n",
    "print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc, True)\n",
    "print(query_doc_bow)\n",
    "\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "\n",
    "\n",
    "\n",
    "listnew =[]\n",
    "listnew = list(zip(keyvaluePair,sims[query_doc_tf_idf]))\n",
    "\n",
    "listnew.sort(key = operator.itemgetter(1),reverse = True)\n",
    "\n",
    "for i in range(5):\n",
    "    print(listnew[i])\n",
    "\n",
    "\n",
    "\"\"\" a = entry.get(\"Segment Title\")\n",
    "a.replace('.',\" \")\n",
    "b = entry.get(\"Segment Definition\")\n",
    "b.replace('.',\" \")\n",
    "c = entry.get(\"Family Title\")\n",
    "c.replace('.',\" \")\n",
    "d =entry.get(\"Family Definition\")\n",
    "d.replace('.',\" \")  \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
