{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from gensim import corpora\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import TfidfModel\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from xlrd import open_workbook\n",
    "import operator\n",
    "import json\n",
    "import pandas as pd\n",
    "from gensim.models import LsiModel\n",
    "from gensim import similarities\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.corpora import MmCorpus\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n"
     ]
    }
   ],
   "source": [
    "book = open_workbook('ignore/UNSPSC English v220601 project.xlsx')\n",
    "'''book = open_workbook('ignore/Unspec List2b.xlsx')'''\n",
    "'''To work on the UNSPSC sheet you need to change the values of 0 to 12 and 1 to\n",
    "16 in order to make the it work.'''\n",
    "\n",
    "dict_list = []\n",
    "sheet = book.sheet_by_index(0)\n",
    "# read header values into the list\n",
    "keys = [sheet.cell(0, col_index).value for col_index in range(sheet.ncols)]\n",
    "\n",
    "for row_index in range(1, sheet.nrows):\n",
    "    d = {keys[col_index]: sheet.cell(row_index, col_index).value\n",
    "         for col_index in range(sheet.ncols)}\n",
    "    dict_list.append(d)\n",
    "\n",
    "listOfEntry = []\n",
    "FamilyList = []\n",
    "sortlist=[]\n",
    "sort = []\n",
    "sen = []\n",
    "counter = 1\n",
    "num=1\n",
    "\n",
    "path = \"UnSpec Family\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "\n",
    "for entry in dict_list:\n",
    "    if(entry.get(\"Family\")==\"\"):\n",
    "        continue\n",
    "    \n",
    "    if(entry.get(\"Family\")!=\"\" and entry.get(\"Class\")==\"\"):\n",
    "        #if(counter%2==0):\n",
    "        if(listOfEntry!=[]):\n",
    "            f = open(os.path.join(path,\"family{0}.txt\".format(num)),\"w\")\n",
    "            sortlist.append(sort)\n",
    "            for line in listOfEntry:\n",
    "              hold = str(line,).strip('[]')\n",
    "              hold = hold.replace(\"'\",\"\")\n",
    "              #hold = str(str(line).strip('[]'),encoding='utf-8')\n",
    "              f.write(json.dumps(hold))\n",
    "              f.write(\"\\n\")\n",
    "            #f.writelines(json.dumps(listOfEntry))\n",
    "            counter += 1\n",
    "            f.close()\n",
    "            listOfEntry.clear()\n",
    "            sort=[]\n",
    "            continue\n",
    "\n",
    "        #else:\n",
    "         #   counter += 1\n",
    "        \n",
    "    else:\n",
    "        e = entry.get(\"Class Title\")\n",
    "        f = entry.get(\"Class Definition\")\n",
    "        g = entry.get(\"Commodity Title\")\n",
    "        h = entry.get(\"Commodity Definition\")\n",
    "        i = entry.get(\"Family\")\n",
    "        j = entry.get(\"Commodity\")\n",
    "        if(f != \"\" and h != \"\"):\n",
    "            result = e+\". \"+f+\". \"+g+\". \"+h+\". \"\n",
    "        elif(f != \"\" and h == \"\"):\n",
    "            result = e+\". \"+f+\". \"+g+\". \"+h\n",
    "        elif(f == \"\" and h != \"\"):\n",
    "            result = e+\". \"+f+\" \"+g+\". \"+h+\". \"\n",
    "        else:\n",
    "            result = e + \". \" + f + \"\" + g + \". \" + h\n",
    "        sen = sent_tokenize(result.lower())\n",
    "        listOfEntry.append(sen)\n",
    "        sort.append(j)\n",
    "        num=int(i)\n",
    "        e, f, g, h = \"\", \"\", \"\", \"\"\n",
    "\n",
    "        \n",
    "f = open(os.path.join(path,\"family{0}.txt\".format(num)),\"w+\")\n",
    "sortlist.append(sort)\n",
    "for line in listOfEntry:\n",
    "    hold = str(line,).strip('[]')\n",
    "    hold = hold.replace(\"'\",\"\")\n",
    "    f.write(json.dumps(hold))\n",
    "    f.write(\"\\n\")\n",
    "counter += 1\n",
    "f.close()\n",
    "        \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file_docs = []\n",
    "\n",
    "\n",
    "anotherCount = 1\n",
    "df1 = pd.DataFrame()\n",
    "df2 = []\n",
    "fam = []\n",
    "path1 = os.path.join(path)\n",
    "for file in os.listdir(path1):\n",
    "    with open(os.path.join(path,file),'r', encoding='utf-8') as infile:\n",
    "        txt = infile.readlines()\n",
    "        for i in txt:\n",
    "            fam.append(i) \n",
    "        df1 = df1.append(fam)\n",
    "        df2.append(df1)\n",
    "        df1 = pd.DataFrame()\n",
    "        fam.clear()\n",
    "        anotherCount += 1\n",
    "       \n",
    "alist =[]  \n",
    "\n",
    "for data in df2:\n",
    "    alist.append(data[0].tolist())\n",
    "    \n",
    "print(len(alist))\n",
    "\n",
    "token =[]\n",
    "\n",
    "\n",
    "Corlist=[]\n",
    "Dictlist=[]\n",
    "lengths=[]\n",
    "inc = 1\n",
    "ch = [ '``','``', \"''\",',','.','\\\\n',\"'\",\";\",\":\",\"(\",\")\",\"-\",\"--\"]\n",
    "for input in alist:\n",
    "    for j in input:\n",
    "        tokens = word_tokenize(j)\n",
    "        tokens = [ j for j in tokens if not j in ch ]\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        tokens = [p.replace('.',\"\") for p in tokens]\n",
    "        token.append(tokens)\n",
    "    out_fname = get_tmpfile(\"corpus{0}\".format(inc))\n",
    "    tmp_fname = get_tmpfile(\"dictionary{0}\".format(inc))\n",
    "    lengths.append(len(token))\n",
    "    inc += 1\n",
    "    dictionary = corpora.Dictionary(token)\n",
    "    corpus = [dictionary.doc2bow(entry) for entry in token]\n",
    "    token.clear()\n",
    "    tf_idf = TfidfModel(corpus)\n",
    "    cor_tf = tf_idf[corpus]\n",
    "    MmCorpus.serialize(out_fname,cor_tf)\n",
    "    Corlist.append(out_fname)\n",
    "    dictionary.save_as_text(tmp_fname)\n",
    "    Dictlist.append(tmp_fname)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = \"MASS TRANSPORTATION - RAIL VEHICLE PARTS AND ACCESSORIES\"\n",
    "test = test.lower()\n",
    "query_doc = word_tokenize(test)\n",
    "\n",
    "\n",
    "listNew = []\n",
    "listNew2 =[]\n",
    "\n",
    "def reduceList(lengths,sortlist, dictionarys,corpuses):\n",
    "    for i in range(len(lengths)):\n",
    "        mm = MmCorpus(corpuses[i])\n",
    "        load_dic = corpora.Dictionary.load_from_text(dictionarys[i])\n",
    "        lsi = LsiModel(mm,num_topics=lengths[i],id2word = load_dic)\n",
    "        index = gensim.similarities.MatrixSimilarity(lsi[mm],num_features=lengths[i])\n",
    "        query_doc_bow = load_dic.doc2bow(query_doc, True)\n",
    "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        sim = list(zip(sortlist[i],index[lsi[query_doc_tf_idf]]))\n",
    "        sim.sort(key = operator.itemgetter(1),reverse = True)\n",
    "        for i,v in sim:\n",
    "            if(v>=0.1):\n",
    "                listNew.append((i,v))\n",
    "        \n",
    "\n",
    "\n",
    "reduceList(lengths, sortlist,Dictlist,Corlist)\n",
    "    \n",
    "       \n",
    "\n",
    "exceltest = pd.ExcelWriter(\"test.xlsx\",engine='xlsxwriter')\n",
    "data= pd.DataFrame(listNew)\n",
    "data.to_excel(exceltest,sheet_name='MASS TRANSPORTATION')\n",
    "\n",
    "exceltest.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
