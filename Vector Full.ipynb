{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from gensim import corpora\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import TfidfModel\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from xlrd import open_workbook\n",
    "import operator\n",
    "import json\n",
    "import pandas as pd\n",
    "from gensim.models import LsiModel\n",
    "from gensim import similarities\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.corpora import MmCorpus\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n",
      "Made the files and temp dictionary and corpus in 462.1494264602661\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "book = open_workbook('ignore/UNSPSC English v220601 project.xlsx')\n",
    "'''book = open_workbook('ignore/Unspec List2b.xlsx')'''\n",
    "'''To work on the UNSPSC sheet you need to change the values of 0 to 12 and 1 to\n",
    "16 in order to make the it work.'''\n",
    "dict_list = []\n",
    "sheet = book.sheet_by_index(0)\n",
    "# read header values into the list\n",
    "keys = [sheet.cell(0, col_index).value for col_index in range(sheet.ncols)]\n",
    "\n",
    "for row_index in range(1, sheet.nrows):\n",
    "    d = {keys[col_index]: sheet.cell(row_index, col_index).value\n",
    "         for col_index in range(sheet.ncols)}\n",
    "    dict_list.append(d)\n",
    "\n",
    "listOfEntry = []\n",
    "FamilyList = []\n",
    "sen = []\n",
    "sortlist=[]\n",
    "sort = []\n",
    "keylist=[]\n",
    "counter = 1\n",
    "num=1\n",
    "\n",
    "path = \"UnSpec Family\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "\n",
    "for entry in dict_list:\n",
    "    if(entry.get(\"Family\")==\"\"):\n",
    "        continue\n",
    "    \n",
    "    if(entry.get(\"Family\")!=\"\" and entry.get(\"Class\")==\"\"):\n",
    "        #if(counter%2==0):\n",
    "        if(listOfEntry!=[]):\n",
    "            f = open(os.path.join(path,\"family{0}.txt\".format(num)),\"w\")\n",
    "            sortlist.append(sort)\n",
    "            for line in listOfEntry:\n",
    "              hold = str(line,).strip('[]')\n",
    "              hold = hold.replace(\"'\",\"\")\n",
    "              #hold = str(str(line).strip('[]'),encoding='utf-8')\n",
    "              f.write(json.dumps(hold))\n",
    "              f.write(\"\\n\")\n",
    "            #f.writelines(json.dumps(listOfEntry))\n",
    "            counter += 1\n",
    "            f.close()\n",
    "            listOfEntry.clear()\n",
    "            continue\n",
    "\n",
    "        #else:\n",
    "         #   counter += 1\n",
    "        \n",
    "    else:\n",
    "        e = entry.get(\"Class Title\")\n",
    "        f = entry.get(\"Class Definition\")\n",
    "        g = entry.get(\"Commodity Title\")\n",
    "        h = entry.get(\"Commodity Definition\")\n",
    "        i = entry.get(\"Family\")\n",
    "        j= entry.get(\"Commodity\")\n",
    "        if(f != \"\" and h != \"\"):\n",
    "            result = e+\". \"+f+\". \"+g+\". \"+h+\". \"\n",
    "        elif(f != \"\" and h == \"\"):\n",
    "            result = e+\". \"+f+\". \"+g+\". \"+h\n",
    "        elif(f == \"\" and h != \"\"):\n",
    "            result = e+\". \"+f+\" \"+g+\". \"+h+\". \"\n",
    "        else:\n",
    "            result = e + \". \" + f + \"\" + g + \". \" + h\n",
    "        sen = sent_tokenize(result.lower())\n",
    "        listOfEntry.append(sen)\n",
    "        sort.append(j)\n",
    "        num=int(i)\n",
    "        e, f, g, h = \"\", \"\", \"\", \"\"\n",
    "\n",
    "        \n",
    "f = open(os.path.join(path,\"family{0}.txt\".format(num)),\"w+\")\n",
    "sortlist.append(sort)\n",
    "for line in listOfEntry:\n",
    "    hold = str(line,).strip('[]')\n",
    "    hold = hold.replace(\"'\",\"\")\n",
    "    f.write(json.dumps(hold))\n",
    "    f.write(\"\\n\")\n",
    "counter += 1\n",
    "f.close()\n",
    "        \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file_docs = []\n",
    "\n",
    "\n",
    "anotherCount = 1\n",
    "df1 = pd.DataFrame()\n",
    "df2 = []\n",
    "fam = []\n",
    "path1 = os.path.join(path)\n",
    "for file in os.listdir(path1):\n",
    "    with open(os.path.join(path,file),'r', encoding='utf-8') as infile:\n",
    "        txt = infile.readlines()\n",
    "        for i in txt:\n",
    "            fam.append(i) \n",
    "        df1 = df1.append(fam)\n",
    "        df2.append(df1)\n",
    "        df1 = pd.DataFrame()\n",
    "        fam.clear()\n",
    "        anotherCount += 1\n",
    "       \n",
    "alist =[]  \n",
    "\n",
    "for data in df2:\n",
    "    alist.append(data[0].tolist())\n",
    "    \n",
    "print(len(alist))\n",
    "\n",
    "token =[]\n",
    "\n",
    "\n",
    "Corlist=[]\n",
    "Dictlist=[]\n",
    "lengths=[]\n",
    "inc = 1\n",
    "ch = [ '``','``', \"''\",',','.','\\\\n',\"'\",\";\",\":\",\"(\",\")\",\"-\",\"--\"]\n",
    "for input in alist:\n",
    "    for j in input:\n",
    "        tokens = word_tokenize(j)\n",
    "        tokens = [ j for j in tokens if not j in ch ]\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        tokens = [p.replace('.',\"\") for p in tokens]\n",
    "        token.append(tokens)\n",
    "    out_fname = get_tmpfile(\"corpus{0}\".format(inc))\n",
    "    tmp_fname = get_tmpfile(\"dictionary{0}\".format(inc))\n",
    "    lengths.append(len(token))\n",
    "    inc += 1\n",
    "    dictionary = corpora.Dictionary(token)\n",
    "    corpus = [dictionary.doc2bow(entry) for entry in token]\n",
    "    token.clear()\n",
    "    tf_idf = TfidfModel(corpus)\n",
    "    cor_tf = tf_idf[corpus]\n",
    "    MmCorpus.serialize(out_fname,cor_tf)\n",
    "    Corlist.append(out_fname)\n",
    "    dictionary.save_as_text(tmp_fname)\n",
    "    Dictlist.append(tmp_fname)\n",
    "    \n",
    "    \n",
    "print(f'Made the files and temp dictionary and corpus in {time.time() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8f3e40e61ed9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mreduceList\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msortlist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDictlist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mCorlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-8f3e40e61ed9>\u001b[0m in \u001b[0;36mreduceList\u001b[1;34m(lengths, sortlist, dictionarys, corpuses)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mload_dic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionarys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mlsi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLsiModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mid2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMatrixSimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlsi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mquery_doc_bow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mquery_doc_tf_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery_doc_bow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\similarities\\docsim.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_best, dtype, num_features, chunksize, corpus_len)\u001b[0m\n\u001b[0;32m    807\u001b[0m                     \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m                     \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse2full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocno\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py\u001b[0m in \u001b[0;36munitvec\u001b[1;34m(vec, norm, return_norm)\u001b[0m\n\u001b[0;32m    758\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mblas_scal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mveclen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mveclen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mblas_scal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mveclen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreturn_norm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = \"MASS TRANSPORTATION - RAIL VEHICLE PARTS AND ACCESSORIES\"\n",
    "test = test.lower()\n",
    "query_doc = word_tokenize(test)\n",
    "\n",
    "\n",
    "listNew = []\n",
    "\n",
    "\n",
    "def reduceList(lengths,sortlist, dictionarys,corpuses):\n",
    "    for i in range(len(lengths)):\n",
    "        mm = MmCorpus(corpuses[i])\n",
    "        load_dic = corpora.Dictionary.load_from_text(dictionarys[i])\n",
    "        lsi = LsiModel(mm,num_topics=lengths[i],id2word = load_dic)\n",
    "        index = gensim.similarities.MatrixSimilarity(lsi[mm],num_features=lengths[i])\n",
    "        query_doc_bow = load_dic.doc2bow(query_doc, True)\n",
    "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        sim = list(zip(sortlist[i],index[lsi[query_doc_tf_idf]]))\n",
    "        sim.sort(key = operator.itemgetter(1),reverse = True)\n",
    "        listNew.append(sim[0:5])\n",
    "        \n",
    "\n",
    "\n",
    "reduceList(lengths, sortlist,Dictlist,Corlist)\n",
    "    \n",
    "       \n",
    "\n",
    "\n",
    "exceltest = pd.ExcelWriter(\"test.xlsx\",engine='xlsxwriter')\n",
    "data= pd.DataFrame(listNew)\n",
    "data.to_excel(exceltest,sheet_name='MASS TRANSPORTATION')\n",
    "\n",
    "exceltest.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
