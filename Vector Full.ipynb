{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from gensim import corpora\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import TfidfModel\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict\n",
    "from os import listdir, makedirs\n",
    "from os.path import isfile, join, exists\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from xlrd import open_workbook\n",
    "import operator\n",
    "import json\n",
    "import pandas as pd\n",
    "from gensim.models import LsiModel\n",
    "from gensim import similarities\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.corpora import MmCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[43, 40, 159]\n",
      "['C:\\\\Users\\\\z12dr\\\\AppData\\\\Local\\\\Temp\\\\dictionary1', 'C:\\\\Users\\\\z12dr\\\\AppData\\\\Local\\\\Temp\\\\dictionary2', 'C:\\\\Users\\\\z12dr\\\\AppData\\\\Local\\\\Temp\\\\dictionary3']\n",
      "['C:\\\\Users\\\\z12dr\\\\AppData\\\\Local\\\\Temp\\\\corpus1', 'C:\\\\Users\\\\z12dr\\\\AppData\\\\Local\\\\Temp\\\\corpus2', 'C:\\\\Users\\\\z12dr\\\\AppData\\\\Local\\\\Temp\\\\corpus3']\n",
      "[['', 30121501.0, 30121503.0, 30121504.0, '', 30121601.0, 30121602.0, 30121603.0, 30121604.0, 30121605.0, '', 30121701.0, 30121702.0, 30121703.0, 30121704.0, 30121705.0, 30121706.0, 30121707.0, 30121708.0, 30121709.0, 30121710.0, 30121711.0, 30121712.0, 30121713.0, 30121714.0, 30121715.0, 30121716.0, 30121717.0, 30121718.0, 30121719.0, 30121720.0, 30121721.0, '', 30121801.0, 30121802.0, 30121803.0, '', 30121901.0, '', 30122001.0, 30122002.0, 30122003.0, 30122004.0], ['', 39101601.0, 39101603.0, 39101605.0, 39101608.0, 39101609.0, 39101612.0, 39101613.0, 39101614.0, 39101615.0, 39101616.0, 39101617.0, 39101618.0, 39101619.0, 39101620.0, 39101621.0, 39101622.0, 39101623.0, 39101624.0, 39101625.0, 39101626.0, 39101627.0, 39101628.0, 39101629.0, '', 39101801.0, 39101802.0, 39101803.0, 39101804.0, 39101805.0, 39101806.0, 39101807.0, '', 39101901.0, 39101902.0, 39101903.0, 39101904.0, 39101905.0, 39101906.0, 39101907.0], ['', 39111501.0, 39111503.0, 39111504.0, 39111505.0, 39111506.0, 39111507.0, 39111508.0, 39111509.0, 39111510.0, 39111512.0, 39111515.0, 39111520.0, 39111521.0, 39111522.0, 39111524.0, 39111525.0, 39111527.0, 39111528.0, 39111529.0, 39111530.0, 39111531.0, 39111532.0, 39111533.0, 39111534.0, 39111535.0, 39111536.0, 39111537.0, 39111538.0, 39111539.0, 39111540.0, 39111541.0, 39111542.0, 39111543.0, 39111544.0, 39111545.0, 39111546.0, 39111547.0, '', 39111603.0, 39111605.0, 39111606.0, 39111608.0, 39111609.0, 39111610.0, 39111611.0, 39111612.0, 39111613.0, 39111614.0, 39111615.0, 39111616.0, 39111617.0, 39111618.0, 39111619.0, 39111620.0, '', 39111703.0, 39111705.0, 39111706.0, 39111707.0, 39111708.0, 39111709.0, 39111710.0, 39111711.0, 39111712.0, 39111713.0, 39111714.0, '', 39111801.0, 39111802.0, 39111803.0, 39111806.0, 39111808.0, 39111809.0, 39111810.0, 39111811.0, 39111812.0, 39111813.0, 39111814.0, 39111815.0, 39111816.0, 39111817.0, 39111818.0, 39111819.0, 39111820.0, 39111821.0, 39111822.0, 39111823.0, 39111824.0, 39111825.0, 39111826.0, 39111827.0, 39111828.0, 39111829.0, 39111830.0, 39111831.0, '', 39111903.0, 39111904.0, 39111905.0, 39111906.0, 39111907.0, 39111908.0, 39111909.0, 39111910.0, 39111911.0, '', 39112001.0, 39112002.0, 39112003.0, 39112004.0, 39112005.0, 39112006.0, 39112007.0, 39112008.0, 39112009.0, 39112010.0, 39112011.0, 39112012.0, 39112013.0, '', 39112101.0, 39112102.0, 39112103.0, '', 39112201.0, 39112202.0, 39112203.0, 39112204.0, 39112205.0, 39112206.0, '', 39112301.0, 39112302.0, 39112303.0, 39112304.0, 39112305.0, 39112306.0, 39112307.0, 39112308.0, 39112309.0, '', 39112401.0, 39112402.0, 39112403.0, '', 39112501.0, 39112502.0, 39112503.0, 39112504.0, 39112505.0, 39112506.0, 39112507.0, 39112508.0, '', 39112601.0, 39112602.0, 39112603.0, 39112604.0]]\n"
     ]
    }
   ],
   "source": [
    "'''book = open_workbook('ignore/UNSPSC English v220601 project.xlsx')'''\n",
    "book = open_workbook('ignore/Unspec List2b.xlsx')\n",
    "'''To work on the UNSPSC sheet you need to change the values of 0 to 12 and 1 to\n",
    "16 in order to make the it work.'''\n",
    "dict_list = []\n",
    "sheet = book.sheet_by_index(0)\n",
    "# read header values into the list\n",
    "keys = [sheet.cell(0, col_index).value for col_index in range(sheet.ncols)]\n",
    "\n",
    "for row_index in range(1, sheet.nrows):\n",
    "    d = {keys[col_index]: sheet.cell(row_index, col_index).value\n",
    "         for col_index in range(sheet.ncols)}\n",
    "    dict_list.append(d)\n",
    "\n",
    "listOfEntry = []\n",
    "FamilyList = []\n",
    "sortlist=[]\n",
    "sort = []\n",
    "sen = []\n",
    "counter = 1\n",
    "num=1\n",
    "\n",
    "path = \"UnSpec Family\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "\n",
    "\n",
    "for entry in dict_list:\n",
    "    if(entry.get(\"Family\")==\"\"):\n",
    "        continue\n",
    "    \n",
    "    if(entry.get(\"Family\")!=\"\" and entry.get(\"Class\")==\"\"):\n",
    "        #if(counter%2==0):\n",
    "        if(listOfEntry!=[]):\n",
    "            f = open(os.path.join(path,\"family{0}.txt\".format(num)),\"w\")\n",
    "            sortlist.append(sort)\n",
    "            for line in listOfEntry:\n",
    "              hold = str(line,).strip('[]')\n",
    "              hold = hold.replace(\"'\",\"\")\n",
    "              #hold = str(str(line).strip('[]'),encoding='utf-8')\n",
    "              f.write(json.dumps(hold))\n",
    "              f.write(\"\\n\")\n",
    "            #f.writelines(json.dumps(listOfEntry))\n",
    "            counter += 1\n",
    "            f.close()\n",
    "            listOfEntry.clear()\n",
    "            sort=[]\n",
    "            continue\n",
    "\n",
    "        #else:\n",
    "         #   counter += 1\n",
    "        \n",
    "    else:\n",
    "        e = entry.get(\"Class Title\")\n",
    "        f = entry.get(\"Class Definition\")\n",
    "        g = entry.get(\"Commodity Title\")\n",
    "        h = entry.get(\"Commodity Definition\")\n",
    "        i = entry.get(\"Family\")\n",
    "        j = entry.get(\"Commodity\")\n",
    "        if(f != \"\" and h != \"\"):\n",
    "            result = e+\". \"+f+\". \"+g+\". \"+h+\". \"\n",
    "        elif(f != \"\" and h == \"\"):\n",
    "            result = e+\". \"+f+\". \"+g+\". \"+h\n",
    "        elif(f == \"\" and h != \"\"):\n",
    "            result = e+\". \"+f+\" \"+g+\". \"+h+\". \"\n",
    "        else:\n",
    "            result = e + \". \" + f + \"\" + g + \". \" + h\n",
    "        sen = sent_tokenize(result.lower())\n",
    "        listOfEntry.append(sen)\n",
    "        sort.append(j)\n",
    "        num=int(i)\n",
    "        e, f, g, h = \"\", \"\", \"\", \"\"\n",
    "\n",
    "        \n",
    "f = open(os.path.join(path,\"family{0}.txt\".format(num)),\"w+\")\n",
    "sortlist.append(sort)\n",
    "for line in listOfEntry:\n",
    "    hold = str(line,).strip('[]')\n",
    "    hold = hold.replace(\"'\",\"\")\n",
    "    f.write(json.dumps(hold))\n",
    "    f.write(\"\\n\")\n",
    "counter += 1\n",
    "f.close()\n",
    "        \n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "file_docs = []\n",
    "\n",
    "\n",
    "anotherCount = 1\n",
    "df1 = pd.DataFrame()\n",
    "df2 = []\n",
    "fam = []\n",
    "path1 = os.path.join(path)\n",
    "for file in os.listdir(path1):\n",
    "    with open(os.path.join(path,file),'r', encoding='utf-8') as infile:\n",
    "        txt = infile.readlines()\n",
    "        for i in txt:\n",
    "            fam.append(i) \n",
    "        df1 = df1.append(fam)\n",
    "        df2.append(df1)\n",
    "        df1 = pd.DataFrame()\n",
    "        fam.clear()\n",
    "        anotherCount += 1\n",
    "       \n",
    "alist =[]  \n",
    "\n",
    "for data in df2:\n",
    "    alist.append(data[0].tolist())\n",
    "    \n",
    "print(len(alist))\n",
    "\n",
    "token =[]\n",
    "\n",
    "\n",
    "Corlist=[]\n",
    "Dictlist=[]\n",
    "lengths=[]\n",
    "inc = 1\n",
    "ch = [ '``','``', \"''\",',','.','\\\\n',\"'\",\";\",\":\",\"(\",\")\",\"-\",\"--\"]\n",
    "for input in alist:\n",
    "    for j in input:\n",
    "        tokens = word_tokenize(j)\n",
    "        tokens = [ j for j in tokens if not j in ch ]\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        tokens = [p.replace('.',\"\") for p in tokens]\n",
    "        token.append(tokens)\n",
    "    out_fname = get_tmpfile(\"corpus{0}\".format(inc))\n",
    "    tmp_fname = get_tmpfile(\"dictionary{0}\".format(inc))\n",
    "    lengths.append(len(token))\n",
    "    inc += 1\n",
    "    dictionary = corpora.Dictionary(token)\n",
    "    corpus = [dictionary.doc2bow(entry) for entry in token]\n",
    "    token.clear()\n",
    "    tf_idf = TfidfModel(corpus)\n",
    "    cor_tf = tf_idf[corpus]\n",
    "    MmCorpus.serialize(out_fname,cor_tf)\n",
    "    Corlist.append(out_fname)\n",
    "    dictionary.save_as_text(tmp_fname)\n",
    "    Dictlist.append(tmp_fname)\n",
    "    \n",
    "    \n",
    "    \n",
    "print(lengths)\n",
    "print(Dictlist)\n",
    "print(Corlist)\n",
    "\n",
    "print(sortlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(30121703.0, 0.9248624), (30121714.0, 1.4901161e-08), (30121709.0, 1.1175871e-08), (30121705.0, 7.450581e-09), (30121715.0, 7.450581e-09)]\n",
      "[(13, 0.9248624), (24, 1.4901161e-08), (19, 1.1175871e-08), (15, 7.450581e-09), (25, 7.450581e-09)]\n",
      "[('', 0.0), (39101601.0, 0.0), (39101603.0, 0.0), (39101605.0, 0.0), (39101608.0, 0.0)]\n",
      "[(0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0)]\n",
      "[(39111616.0, 0.5442184), (39111541.0, 0.48754525), (39112013.0, 0.43974388), (39111821.0, 0.40579844), (39111909.0, 0.37342858)]\n",
      "[(50, 0.5442184), (31, 0.48754525), (119, 0.43974388), (85, 0.40579844), (103, 0.37342858)]\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z12dr\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:779: RuntimeWarning: overflow encountered in double_scalars\n",
      "  length = 1.0 * math.sqrt(sum(val ** 2 for _, val in vec))\n"
     ]
    }
   ],
   "source": [
    "test = \"MASS TRANSPORTATION - RAIL VEHICLE PARTS AND ACCESSORIES\"\n",
    "test = test.lower()\n",
    "query_doc = word_tokenize(test)\n",
    "\n",
    "\n",
    "listNew = []\n",
    "listNew2 =[]\n",
    "\n",
    "def reduceList(lengths,sortlist, dictionarys,corpuses):\n",
    "    for i in range(len(lengths)):\n",
    "        mm = MmCorpus(corpuses[i])\n",
    "        load_dic = corpora.Dictionary.load_from_text(dictionarys[i])\n",
    "        lsi = LsiModel(mm,num_topics=lengths[i],id2word = load_dic)\n",
    "        index = gensim.similarities.MatrixSimilarity(lsi[mm],num_features=lengths[i])\n",
    "        query_doc_bow = load_dic.doc2bow(query_doc, True)\n",
    "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        sim2 = index[lsi[query_doc_tf_idf]]\n",
    "        sim = list(zip(sortlist[i],index[lsi[query_doc_tf_idf]]))\n",
    "        sim.sort(key = operator.itemgetter(1),reverse = True)\n",
    "        sim2 = sorted(enumerate(sim2), key=lambda item: -item[1])\n",
    "        listNew2.append(sim2[0:5])\n",
    "        listNew.append(sim[0:5])\n",
    "        print(sim[0:5])\n",
    "        print(sim2[0:5])\n",
    "\n",
    "\n",
    "reduceList(lengths, sortlist,Dictlist,Corlist)\n",
    "    \n",
    "       \n",
    "\n",
    "print(len(listNew))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 0.5442184),\n",
       " (31, 0.48754525),\n",
       " (119, 0.43974388),\n",
       " (85, 0.40579844),\n",
       " (103, 0.37342858)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(13, 0.4744661), (4, 0.37624702), (41, 0.16147709), (18, 0.14059983), (40, 0.099389255)]\n",
    "[(17, 0.45512533), (24, 0.42233905), (25, 0.2707626), (26, 0.23992425), (16, 0.20316647)]\n",
    "[(50, 0.5442184), (31, 0.48754525), (119, 0.43974388), (85, 0.40579844), (103, 0.37342858)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(30121703.0, 0.9248624), (30121714.0, 1.4901161e-08), (30121709.0, 1.1175871e-08), (30121705.0, 7.450581e-09), (30121715.0, 7.450581e-09)], [('', 0.0), (39101601.0, 0.0), (39101603.0, 0.0), (39101605.0, 0.0), (39101608.0, 0.0)], [(39111616.0, 0.5442184), (39111541.0, 0.48754525), (39112013.0, 0.43974388), (39111821.0, 0.40579844), (39111909.0, 0.37342858)]]\n"
     ]
    }
   ],
   "source": [
    "print(listNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
